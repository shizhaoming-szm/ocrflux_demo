#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OCRFluxé€é¡µJSONLå¤„ç†åŠŸèƒ½æµ‹è¯•è„šæœ¬

è¯¥è„šæœ¬ç”¨äºæµ‹è¯•æ–°å®ç°çš„é€é¡µå¤„ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. é€é¡µPDFå¤„ç†å¹¶ç”ŸæˆJSONLæ ¼å¼æ•°æ®
2. ç›¸é‚»é¡µé¢è¡¨æ ¼å’Œæ®µè½çš„æ™ºèƒ½åˆå¹¶
3. æœ€ç»ˆMarkdownæ–‡æ¡£çš„ç”Ÿæˆ

ä½¿ç”¨æ–¹æ³•ï¼š
    python test_jsonl_processing.py --pdf_path ./test.pdf --model_path Qwen/Qwen2.5-VL-7B-Instruct

ä½œè€…: OCRFluxå›¢é˜Ÿ
æ—¥æœŸ: 2024
"""

import argparse
import json
import os
import time
from pathlib import Path

try:
    from vllm import LLM
    from ocrflux.inference import parse_page_by_page_jsonl, parse
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…vllmå’Œocrfluxä¾èµ–")
    exit(1)


def test_jsonl_processing(pdf_path, model_path, max_retries=2, output_dir="./test_output"):
    """
    æµ‹è¯•é€é¡µJSONLå¤„ç†åŠŸèƒ½
    
    Args:
        pdf_path (str): PDFæ–‡ä»¶è·¯å¾„
        model_path (str): æ¨¡å‹è·¯å¾„
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°
        output_dir (str): è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    print("=" * 60)
    print("OCRFluxé€é¡µJSONLå¤„ç†åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(pdf_path):
        print(f"é”™è¯¯: PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return False
    
    print(f"PDFæ–‡ä»¶: {pdf_path}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æœ€å¤§é‡è¯•æ¬¡æ•°: {max_retries}")
    print()
    
    try:
        # åˆå§‹åŒ–vLLMæ¨ç†å¼•æ“
        print("æ­£åœ¨åˆå§‹åŒ–vLLMæ¨ç†å¼•æ“...")
        start_time = time.time()
        llm = LLM(model=model_path, max_model_len=8192)
        init_time = time.time() - start_time
        print(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.2f}ç§’")
        print()
        
        # æµ‹è¯•åŸå§‹æ‰¹é‡å¤„ç†æ–¹å¼
        print("1. æµ‹è¯•åŸå§‹æ‰¹é‡å¤„ç†æ–¹å¼")
        print("-" * 40)
        start_time = time.time()
        
        result_batch = parse(llm, pdf_path, max_page_retries=max_retries)
        batch_time = time.time() - start_time
        
        if result_batch:
            batch_output_path = os.path.join(output_dir, "result_batch.md")
            with open(batch_output_path, "w", encoding="utf-8") as f:
                f.write(result_batch["document_text"])
            
            print(f"âœ“ æ‰¹é‡å¤„ç†æˆåŠŸ")
            print(f"  - æ€»é¡µæ•°: {result_batch['num_pages']}")
            print(f"  - å¤±è´¥é¡µé¢: {result_batch['fallback_pages']}")
            print(f"  - å¤„ç†æ—¶é—´: {batch_time:.2f}ç§’")
            print(f"  - è¾“å‡ºæ–‡ä»¶: {batch_output_path}")
        else:
            print("âœ— æ‰¹é‡å¤„ç†å¤±è´¥")
            return False
        
        print()
        
        # æµ‹è¯•æ–°çš„é€é¡µJSONLå¤„ç†æ–¹å¼
        print("2. æµ‹è¯•é€é¡µJSONLå¤„ç†æ–¹å¼")
        print("-" * 40)
        start_time = time.time()
        
        result_jsonl = parse_page_by_page_jsonl(llm, pdf_path, max_page_retries=max_retries)
        jsonl_time = time.time() - start_time
        
        if result_jsonl:
            # ä¿å­˜æœ€ç»ˆMarkdown
            jsonl_md_path = os.path.join(output_dir, "result_jsonl.md")
            with open(jsonl_md_path, "w", encoding="utf-8") as f:
                f.write(result_jsonl["final_markdown"])
            
            # ä¿å­˜è¯¦ç»†çš„JSONLæ•°æ®
            jsonl_data_path = os.path.join(output_dir, "result_jsonl_data.json")
            with open(jsonl_data_path, "w", encoding="utf-8") as f:
                json.dump(result_jsonl, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æ¯é¡µçš„JSONLæ•°æ®
            pages_dir = os.path.join(output_dir, "pages_jsonl")
            os.makedirs(pages_dir, exist_ok=True)
            for page_num, page_data in result_jsonl["page_jsonl_results"].items():
                page_file = os.path.join(pages_dir, f"page_{page_num}.json")
                with open(page_file, "w", encoding="utf-8") as f:
                    json.dump(page_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ“ é€é¡µJSONLå¤„ç†æˆåŠŸ")
            print(f"  - æ€»é¡µæ•°: {result_jsonl['num_pages']}")
            print(f"  - å¤±è´¥é¡µé¢: {result_jsonl['fallback_pages']}")
            print(f"  - åˆå¹¶é¡µé¢å¯¹: {len(result_jsonl['merged_elements'])}")
            print(f"  - å¤„ç†æ—¶é—´: {jsonl_time:.2f}ç§’")
            print(f"  - Markdownè¾“å‡º: {jsonl_md_path}")
            print(f"  - è¯¦ç»†æ•°æ®: {jsonl_data_path}")
            print(f"  - åˆ†é¡µæ•°æ®: {pages_dir}/")
            
            # æ˜¾ç¤ºåˆå¹¶ä¿¡æ¯
            if result_jsonl['merged_elements']:
                print("  - åˆå¹¶è¯¦æƒ…:")
                for (p1, p2), merge_info in result_jsonl['merged_elements'].items():
                    merge_pairs = merge_info.get('merge_pairs', [])
                    print(f"    é¡µé¢ {p1}-{p2}: {len(merge_pairs)} ä¸ªåˆå¹¶é¡¹")
                    for pair in merge_pairs:
                        print(f"      - {pair['type']} (ç½®ä¿¡åº¦: {pair.get('confidence', 'N/A')})")
            
        else:
            print("âœ— é€é¡µJSONLå¤„ç†å¤±è´¥")
            return False
        
        print()
        
        # æ€§èƒ½å¯¹æ¯”
        print("3. æ€§èƒ½å¯¹æ¯”")
        print("-" * 40)
        print(f"æ‰¹é‡å¤„ç†æ—¶é—´: {batch_time:.2f}ç§’")
        print(f"é€é¡µå¤„ç†æ—¶é—´: {jsonl_time:.2f}ç§’")
        print(f"æ—¶é—´å·®å¼‚: {jsonl_time - batch_time:+.2f}ç§’")
        
        if batch_time > 0:
            speed_ratio = jsonl_time / batch_time
            print(f"é€Ÿåº¦æ¯”ç‡: {speed_ratio:.2f}x (>1è¡¨ç¤ºé€é¡µå¤„ç†æ›´æ…¢)")
        
        print()
        
        # ç»“æœå¯¹æ¯”
        print("4. ç»“æœå¯¹æ¯”")
        print("-" * 40)
        batch_length = len(result_batch["document_text"])
        jsonl_length = len(result_jsonl["final_markdown"])
        
        print(f"æ‰¹é‡å¤„ç†è¾“å‡ºé•¿åº¦: {batch_length} å­—ç¬¦")
        print(f"é€é¡µå¤„ç†è¾“å‡ºé•¿åº¦: {jsonl_length} å­—ç¬¦")
        print(f"é•¿åº¦å·®å¼‚: {jsonl_length - batch_length:+d} å­—ç¬¦")
        
        # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
        report_path = os.path.join(output_dir, "comparison_report.txt")
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("OCRFluxå¤„ç†æ–¹å¼å¯¹æ¯”æŠ¥å‘Š\n")
            f.write("=" * 40 + "\n\n")
            f.write(f"PDFæ–‡ä»¶: {pdf_path}\n")
            f.write(f"æ¨¡å‹: {model_path}\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("æ‰¹é‡å¤„ç†ç»“æœ:\n")
            f.write(f"  - æ€»é¡µæ•°: {result_batch['num_pages']}\n")
            f.write(f"  - å¤±è´¥é¡µé¢: {result_batch['fallback_pages']}\n")
            f.write(f"  - å¤„ç†æ—¶é—´: {batch_time:.2f}ç§’\n")
            f.write(f"  - è¾“å‡ºé•¿åº¦: {batch_length} å­—ç¬¦\n\n")
            
            f.write("é€é¡µJSONLå¤„ç†ç»“æœ:\n")
            f.write(f"  - æ€»é¡µæ•°: {result_jsonl['num_pages']}\n")
            f.write(f"  - å¤±è´¥é¡µé¢: {result_jsonl['fallback_pages']}\n")
            f.write(f"  - åˆå¹¶é¡µé¢å¯¹: {len(result_jsonl['merged_elements'])}\n")
            f.write(f"  - å¤„ç†æ—¶é—´: {jsonl_time:.2f}ç§’\n")
            f.write(f"  - è¾“å‡ºé•¿åº¦: {jsonl_length} å­—ç¬¦\n\n")
            
            if result_jsonl['merged_elements']:
                f.write("åˆå¹¶è¯¦æƒ…:\n")
                for (p1, p2), merge_info in result_jsonl['merged_elements'].items():
                    merge_pairs = merge_info.get('merge_pairs', [])
                    f.write(f"  é¡µé¢ {p1}-{p2}: {len(merge_pairs)} ä¸ªåˆå¹¶é¡¹\n")
                    for pair in merge_pairs:
                        f.write(f"    - {pair['type']} (ç½®ä¿¡åº¦: {pair.get('confidence', 'N/A')})\n")
        
        print(f"å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print()
        print("âœ“ æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œæµ‹è¯•
    """
    parser = argparse.ArgumentParser(
        description="OCRFluxé€é¡µJSONLå¤„ç†åŠŸèƒ½æµ‹è¯•è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python test_jsonl_processing.py --pdf_path ./test.pdf
  python test_jsonl_processing.py --pdf_path ./document.pdf --model_path Qwen/Qwen2.5-VL-7B-Instruct --max_retries 3
  python test_jsonl_processing.py --pdf_path ./sample.pdf --output_dir ./my_output
        """
    )
    
    parser.add_argument(
        "--pdf_path",
        type=str,
        default="./test.pdf",
        help="PDFæ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./test.pdf)"
    )
    
    parser.add_argument(
        "--model_path",
        type=str,
        default="Qwen/Qwen2.5-VL-7B-Instruct",
        help="æ¨¡å‹è·¯å¾„ (é»˜è®¤: Qwen/Qwen2.5-VL-7B-Instruct)"
    )
    
    parser.add_argument(
        "--max_retries",
        type=int,
        default=2,
        help="æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 2)"
    )
    
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./test_output",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ./test_output)"
    )
    
    args = parser.parse_args()
    
    # æ‰§è¡Œæµ‹è¯•
    success = test_jsonl_processing(
        pdf_path=args.pdf_path,
        model_path=args.model_path,
        max_retries=args.max_retries,
        output_dir=args.output_dir
    )
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        exit(0)
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼")
        exit(1)


if __name__ == "__main__":
    main()